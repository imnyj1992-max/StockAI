# Stage 3 — Reinforcement Learning for Real-Time Virtual Trading

이 단계에서는 Stage 2에서 제작한 가상 매매 아이디어를 확장하여, 강화학습 에이전트가 실시간으로 가격 변화를 관찰하고 **매수 / 매도 / 관망** 액션을 학습하도록 구현합니다. PyQt5 기반 GUI에서 강화학습 과정을 시각적으로 모니터링할 수 있습니다.

## 주요 구성 요소

- **TradingEnvironment**: 랜덤 워크 기반 가격 시뮬레이터와 단일 종목 포지션(최대 1주)을 관리합니다. 보유 현금, 평가 손익, 행동 결과를 추적합니다.
- **QLearningAgent**: 최근 수익률 패턴을 이산화(discretize)한 상태(state)를 기반으로 Q-Learning을 수행합니다. 에이전트는 탐험/활용 전략(ε-greedy)을 사용하여 의사결정을 합니다.
- **ReinforcementWindow**: PyQt5 GUI로, 학습 시작/일시정지/리셋을 제어하고, 현재 에피소드, 스텝, 에이전트 상태, 누적 보상 등을 실시간으로 표시합니다.

## 실행 방법

```bash
pip install PyQt5 numpy
python stage3_reinforcement_trading/main.py
```

> **참고:** PyQt5 GUI는 데스크탑 환경에서 실행해야 하며, WSL/리눅스 서버 환경에서는 X 서버 설정이 필요할 수 있습니다.

## 학습 파라미터

GUI에서 초기 현금, 한 에피소드당 최대 스텝 수, 상태로 활용할 가격 변동 이력(window) 등을 조정할 수 있습니다. 기본 값은 다음과 같습니다.

- 초기 현금: 10,000 KRW
- 에피소드 스텝 수: 200
- 가격 변동 이력 길이: 3
- 학습률(α): 0.1
- 할인율(γ): 0.95
- 탐험률(ε): 0.2 (에피소드 종료마다 감소)

## 동작 방식 요약

1. **환경 초기화**: 무작위 가격 시뮬레이터를 사용하여 시작 가격을 생성합니다.
2. **에이전트 행동 선택**: ε-greedy 정책으로 BUY / SELL / HOLD 중 하나를 선택합니다.
3. **보상 계산**: 보유 포지션에 따른 평가손익(P&L)을 보상으로 사용하며, 잘못된 행동(보유 주식 없이 매도 등)에는 패널티가 부여됩니다.
4. **Q-Table 업데이트**: 관찰된 상태, 행동, 보상, 다음 상태를 사용하여 Q 값을 갱신합니다.
5. **GUI 업데이트**: 각 스텝마다 가격, 포지션, 보상, 누적 수익 등을 화면에 표시하고 로그에 기록합니다.

에피소드가 종료되면 자동으로 다음 에피소드를 시작하며, ε 값은 최소 ε까지 점진적으로 감소하여 학습 후반부에는 활용 비중이 높아집니다.

## 향후 확장 아이디어

- Stage 2의 가상 브로커와 직접 연동하여 여러 종목, 여러 포지션을 다루도록 확장
- 딥러닝 기반 함수 근사(DQN, Policy Gradient 등)로 상태 공간 확대
- 실시간 데이터 피드(예: 웹소켓)를 사용하여 실제 시장 데이터 기반 강화학습 테스트

이 예제는 강화학습 파이프라인의 기본 구조를 파악하고, 이후 단계에서 더 복잡한 전략을 구현하기 위한 토대를 제공합니다.
